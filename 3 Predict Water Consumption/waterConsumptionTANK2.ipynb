{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport glob\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras import layers\nfrom kerastuner.tuners import RandomSearch\nimport kerastuner as kt\nfrom tensorflow.keras.layers import LSTM, Dense, GRU\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder_path = '/kaggle/input/water-consumption/Household water consumption dataset/Tank 2-Data Files'\nfiles_list = glob.glob(folder_path + '/*.xlsx')\ndfs = []\nfor file in files_list:\n    df = pd.read_excel(file, parse_dates=(['created_at']))\n    dfs.append(df)\n    \nmerged_df = pd.concat(dfs, ignore_index=True)\nmerged_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df['created_at'] = pd.to_datetime(merged_df['created_at'], format='%d/%m/%Y %H:%M')\nmerged_df[\"year\"] = merged_df[\"created_at\"].dt.year\nmerged_df[\"month\"] = merged_df[\"created_at\"].dt.month\nmerged_df[\"day\"] = merged_df[\"created_at\"].dt.day\nmerged_df[\"hour\"] = merged_df[\"created_at\"].dt.hour\nmerged_df[\"minute\"] = merged_df[\"created_at\"].dt.minute\nmerged_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_data = merged_df.isna().sum()\nduplicate_rows = merged_df.duplicated()\nprint('Nulls', null_data)\nprint('Duplicates', duplicate_rows)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detecting outliers using Z-score or IQR\n# Let's focus on the \"field1\" column for outlier detection\nQ1 = merged_df[\"field1\"].quantile(0.25)\nQ3 = merged_df[\"field1\"].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n\n# Identify and remove outliers\ndf_no_outliers = merged_df[\n    (merged_df[\"field1\"].iloc[:] >= lower_bound) &\n    (merged_df[\"field1\"].iloc[:] <= upper_bound)\n]\ndf_no_outliers.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_no_outliers.drop(['field1', 'created_at'], axis=1)\ny = df_no_outliers['field1']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying Standard Scaling to the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape data for LSTM and  gru (samples, timesteps, features)\nX_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\nX_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lstm_model = Sequential([\n#     LSTM(128, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True),\n#     LSTM(64, activation='relu'),\n#     Dense(1)  # Output layer\n# ])\n\n# lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n\n# # # Use early stopping to prevent overfitting\n# # early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# lstm_model.fit(X_train_reshaped, y_train, epochs=100, validation_split=0.2)\n\n# # Evaluate the LSTM model\n# y_pred_lstm = lstm_model.predict(X_test_reshaped)\n\n# # Calculate and print R-squared\n# r_squared_lstm = r2_score(y_test, y_pred_lstm)\n# print(\"LSTM R-squared:\", r_squared_lstm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# r_squared = r2_score(y_test, y_pred_lstm)\n# mae = mean_absolute_error(y_test, y_pred_lstm)\n# rmse = mean_squared_error(y_test, y_pred_lstm, squared=False)\n# mse = mean_squared_error(y_test, y_pred_lstm)\n\n# print(\"TANK2:\")\n# print(\"R-squared:\", r_squared)\n# print(\"Mean Absolute Error:\", mae)\n# print(\"Root Mean Squared Error:\", rmse)\n# print(\"Mean Squared Error:\", mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Define a function to build the LSTM model with hyperparameters\n# def build_model(hp):\n#     model = keras.Sequential()\n#     model.add(LSTM(units=hp.Int('units', min_value=32, max_value=256, step=32),\n#                    activation=hp.Choice('activation', values=['relu', 'tanh']),\n#                    return_sequences=True,\n#                    input_shape=(1, X_train_scaled.shape[1])))\n#     model.add(LSTM(units=hp.Int('units', min_value=32, max_value=128, step=32),\n#                    activation=hp.Choice('activation', values=['relu', 'tanh'])))\n#     model.add(Dense(1))\n\n#     # Compile the model\n#     model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n#                   loss='mse',\n#                   metrics=['mse'])\n#     return model\n\n# # Define the tuner and search space\n# tuner = RandomSearch(\n#     build_model,\n#     objective='val_loss',\n#     max_trials=10,  # Number of trials to run\n#     directory='my_tuning_directory',  # Directory to save results\n#     project_name='my_lstm_tuning'  # Name for this tuning project\n# )\n\n# # Perform hyperparameter tuning\n# tuner.search(X_train_reshaped, y_train, epochs=100, validation_data=(X_test_reshaped, y_test))\n\n# # Get the best hyperparameters\n# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n\n# # Build the best model\n# best_model = tuner.hypermodel.build(best_hps)\n\n# # Train the best model\n# best_model.fit(X_train_reshaped, y_train, epochs=100, validation_data=(X_test_reshaped, y_test))\n\n# # Evaluate the best model\n# y_pred = best_model.predict(X_test_reshaped)\n# mse = mean_squared_error(y_test, y_pred)\n# print(\"Mean Squared Error (MSE) for Best Model:\", mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# r_squared = r2_score(y_test, y_pred)\n# mae = mean_absolute_error(y_test, y_pred)\n# rmse = mean_squared_error(y_test, y_pred, squared=False)\n# mse = mean_squared_error(y_test, y_pred)\n\n# print(\"TANK2:\")\n# print(\"R-squared:\", r_squared)\n# print(\"Mean Absolute Error:\", mae)\n# print(\"Root Mean Squared Error:\", rmse)\n# print(\"Mean Squared Error:\", mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GRU","metadata":{}},{"cell_type":"code","source":"gru_model = Sequential([\n    GRU(128, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True),\n    GRU(64, activation='relu'),\n    Dense(1)  # Output layer\n])\n\ngru_model.compile(optimizer='adam', loss='mean_squared_error')\n\n# Use early stopping to prevent overfitting\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\ngru_model.fit(X_train_reshaped, y_train, epochs=100, validation_split=0.2)\n\n# Evaluate the GRU model\ny_pred_gru = gru_model.predict(X_test_reshaped)\n\n# Calculate and print R-squared\nr_squared_gru = r2_score(y_test, y_pred_gru)\nprint(\"GRU R-squared:\", r_squared_gru)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r_squared = r2_score(y_test, y_pred_gru)\nmae = mean_absolute_error(y_test, y_pred_gru)\nrmse = mean_squared_error(y_test, y_pred_gru, squared=False)\nmse = mean_squared_error(y_test, y_pred_gru)\n\nprint(\"TANK2:\")\nprint(\"R-squared:\", r_squared)\nprint(\"Mean Absolute Error:\", mae)\nprint(\"Root Mean Squared Error:\", rmse)\nprint(\"Mean Squared Error:\", mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}